{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arabic_Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN6nyJkfQC4I",
        "outputId": "6ebbc7fd-655a-4c46-940d-33e7712c6c33"
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /Users/sammy/.local/lib/python3.7/site-packages (1.7.0)\n",
            "Requirement already satisfied: numpy in /Users/sammy/anaconda3/lib/python3.7/site-packages (from torch) (1.17.2)\n",
            "Requirement already satisfied: future in /Users/sammy/anaconda3/lib/python3.7/site-packages (from torch) (0.17.1)\n",
            "Requirement already satisfied: typing-extensions in /Users/sammy/.local/lib/python3.7/site-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /Users/sammy/.local/lib/python3.7/site-packages (from torch) (0.6)\n",
            "Requirement already satisfied: torchvision in /Users/sammy/.local/lib/python3.7/site-packages (0.8.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /Users/sammy/anaconda3/lib/python3.7/site-packages (from torchvision) (6.2.0)\n",
            "Requirement already satisfied: torch==1.7.0 in /Users/sammy/.local/lib/python3.7/site-packages (from torchvision) (1.7.0)\n",
            "Requirement already satisfied: numpy in /Users/sammy/anaconda3/lib/python3.7/site-packages (from torchvision) (1.17.2)\n",
            "Requirement already satisfied: future in /Users/sammy/anaconda3/lib/python3.7/site-packages (from torch==1.7.0->torchvision) (0.17.1)\n",
            "Requirement already satisfied: dataclasses in /Users/sammy/.local/lib/python3.7/site-packages (from torch==1.7.0->torchvision) (0.6)\n",
            "Requirement already satisfied: typing-extensions in /Users/sammy/.local/lib/python3.7/site-packages (from torch==1.7.0->torchvision) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybYvxLmhQHYP",
        "outputId": "7280e023-f499-457d-c4be-adf84966c670"
      },
      "source": [
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "7651z9U3QfHw",
        "outputId": "a0a445f4-b5e9-4d60-85a2-4fab4aef26e8"
      },
      "source": [
        "lines = open('./ara.txt', encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "num_examples = 3000\n",
        "original_word_pairs = [[w for w in l.split('\\t')] for l in lines[:num_examples]]\n",
        "\n",
        "data = pd.DataFrame(original_word_pairs, columns=[\"eng\", \"ar\",\"attr\"])\n",
        "data.head(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     eng       ar                                               attr\n",
              "0    Hi.  مرحبًا.  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
              "1   Run!    اركض!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
              "2  Help!  النجدة!  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
              "3  Jump!    اقفز!  CC-BY 2.0 (France) Attribution: tatoeba.org #1...\n",
              "4  Stop!      قف!  CC-BY 2.0 (France) Attribution: tatoeba.org #4..."
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng</th>\n      <th>ar</th>\n      <th>attr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Hi.</td>\n      <td>مرحبًا.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>Run!</td>\n      <td>اركض!</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>Help!</td>\n      <td>النجدة!</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>Jump!</td>\n      <td>اقفز!</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>Stop!</td>\n      <td>قف!</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L77ukoTxRGKv"
      },
      "source": [
        "import unicodedata as ud\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n",
        "def arabic_preprocess_sentence(w):\n",
        "  # retStr = ''\n",
        "  # for c in w:\n",
        "  #   if not ud.category(c).startswith('P'):\n",
        "  #     retStr = retStr.join(c)\n",
        "  #   else:\n",
        "  #     retStr = ' ' + retStr.join(c) + ' '\n",
        "  # return retStr\n",
        "  w = ''.join(c for c in w if not ud.category(c).startswith('P'))\n",
        "  return '<start>' + w + '<end>'\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YASq-CU7TQga",
        "outputId": "9172338a-09fa-4247-ce8b-940c4b54ad56"
      },
      "source": [
        "# data['ar'].sample(10)\n",
        "\n",
        "data.ar.apply(lambda w: arabic_preprocess_sentence(w)).sample(10)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2611           <start>ماذا حصل هنا<end>\n",
              "1874      <start>إنه يعيش في طوكيو<end>\n",
              "693                  <start>تمشّيت<end>\n",
              "2340    <start>لا يمكنني تصديق ذلك<end>\n",
              "301                <start>مرة أخرى<end>\n",
              "2175               <start>ما الأمر<end>\n",
              "887            <start>كيف حال أبيك<end>\n",
              "1620       <start>إنها على الأريكة<end>\n",
              "1858             <start>لقد وافقني<end>\n",
              "2704      <start>دَمّر هذا المعبَد<end>\n",
              "Name: ar, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "YNS_FNAgRG7-",
        "outputId": "a95ab29c-88e8-493c-a2bb-b7486d1018a9"
      },
      "source": [
        "# Now we do the preprocessing using pandas and lambdas\n",
        "data[\"eng\"] = data.eng.apply(lambda w: preprocess_sentence(w))\n",
        "data[\"ar\"] = data.ar.apply(lambda w: arabic_preprocess_sentence(w))\n",
        "data.sample(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     eng  \\\n",
              "2447  <start> It s raining again ! <end>   \n",
              "642        <start> He s so young . <end>   \n",
              "2185   <start> Where do you live ? <end>   \n",
              "267           <start> I love you . <end>   \n",
              "2656  <start> Why do I even care ? <end>   \n",
              "1438    <start> Come to my house . <end>   \n",
              "532         <start> I m not busy . <end>   \n",
              "2008   <start> It cannot be true . <end>   \n",
              "911       <start> I love lasagna . <end>   \n",
              "1225     <start> I want to drive . <end>   \n",
              "\n",
              "                                          ar  \\\n",
              "2447            <start>إنها تمطر مجدداً<end>   \n",
              "642                <start>إنهُ شاب جداً<end>   \n",
              "2185                    <start>أين تسكن<end>   \n",
              "267                     <start>أنا أحبك<end>   \n",
              "2656          <start>لِمَ أُتْعِبُ نفسي<end>   \n",
              "1438              <start>تعال إلى منزلي<end>   \n",
              "532                  <start>لست مشغولاً<end>   \n",
              "2008  <start>لا يمكن أن يكون ذلك صحيحاً<end>   \n",
              "911            <start>أحب أكل اللازانيا<end>   \n",
              "1225                <start>أريد أن أقود<end>   \n",
              "\n",
              "                                                   attr  \n",
              "2447  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
              "642   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
              "2185  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
              "267   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
              "2656  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
              "1438  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "532   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "2008  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
              "911   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
              "1225  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng</th>\n      <th>ar</th>\n      <th>attr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2447</td>\n      <td>&lt;start&gt; It s raining again ! &lt;end&gt;</td>\n      <td>&lt;start&gt;إنها تمطر مجدداً&lt;end&gt;</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n    </tr>\n    <tr>\n      <td>642</td>\n      <td>&lt;start&gt; He s so young . &lt;end&gt;</td>\n      <td>&lt;start&gt;إنهُ شاب جداً&lt;end&gt;</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n    </tr>\n    <tr>\n      <td>2185</td>\n      <td>&lt;start&gt; Where do you live ? &lt;end&gt;</td>\n      <td>&lt;start&gt;أين تسكن&lt;end&gt;</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n    </tr>\n    <tr>\n      <td>267</td>\n      <td>&lt;start&gt; I love you . &lt;end&gt;</td>\n      <td>&lt;start&gt;أنا أحبك&lt;end&gt;</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n    </tr>\n    <tr>\n      <td>2656</td>\n      <td>&lt;start&gt; Why do I even care ? &lt;end&gt;</td>\n      <td>&lt;start&gt;لِمَ أُتْعِبُ نفسي&lt;end&gt;</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n    </tr>\n    <tr>\n      <td>1438</td>\n      <td>&lt;start&gt; Come to my house . &lt;end&gt;</td>\n      <td>&lt;start&gt;تعال إلى منزلي&lt;end&gt;</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n    <tr>\n      <td>532</td>\n      <td>&lt;start&gt; I m not busy . &lt;end&gt;</td>\n      <td>&lt;start&gt;لست مشغولاً&lt;end&gt;</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n    <tr>\n      <td>2008</td>\n      <td>&lt;start&gt; It cannot be true . &lt;end&gt;</td>\n      <td>&lt;start&gt;لا يمكن أن يكون ذلك صحيحاً&lt;end&gt;</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n    </tr>\n    <tr>\n      <td>911</td>\n      <td>&lt;start&gt; I love lasagna . &lt;end&gt;</td>\n      <td>&lt;start&gt;أحب أكل اللازانيا&lt;end&gt;</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n    </tr>\n    <tr>\n      <td>1225</td>\n      <td>&lt;start&gt; I want to drive . &lt;end&gt;</td>\n      <td>&lt;start&gt;أريد أن أقود&lt;end&gt;</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrxTOWtxRJJK"
      },
      "source": [
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        \"\"\" lang are the list of phrases from each language\"\"\"\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        \n",
        "        self.create_index()\n",
        "        \n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            # update with individual tokens\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "            \n",
        "        # sort the vocab\n",
        "        self.vocab = sorted(self.vocab)\n",
        "\n",
        "        # add a padding token with index 0\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        \n",
        "        # word to index mapping\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1 # +1 because of pad token\n",
        "        \n",
        "        # index to word mapping\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word        "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxT674gjRiij",
        "outputId": "cb578fdd-326b-44d0-8f71-1270cd5f9f68"
      },
      "source": [
        "# index language using the class above\n",
        "inp_lang = LanguageIndex(data[\"ar\"].values.tolist())\n",
        "targ_lang = LanguageIndex(data[\"eng\"].values.tolist())\n",
        "# Vectorize the input and target languages\n",
        "input_tensor = [[inp_lang.word2idx[s] for s in ar.split(' ')]  for ar in data[\"ar\"].values.tolist()]\n",
        "target_tensor = [[targ_lang.word2idx[s] for s in eng.split(' ')]  for eng in data[\"eng\"].values.tolist()]\n",
        "input_tensor[:10]\n",
        "target_tensor[:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 120, 3, 4],\n",
              " [5, 210, 1, 4],\n",
              " [5, 116, 1, 4],\n",
              " [5, 136, 1, 4],\n",
              " [5, 231, 1, 4],\n",
              " [5, 100, 994, 3, 4],\n",
              " [5, 100, 994, 3, 4],\n",
              " [5, 115, 1, 4],\n",
              " [5, 124, 1, 4],\n",
              " [5, 124, 1, 4]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR4p1GVoRoZu"
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD_nv6QORtBm"
      },
      "source": [
        "# calculate the max_length of input and output tensor\n",
        "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB242AjNRu8q"
      },
      "source": [
        "def pad_sequences(x, max_len):\n",
        "    padded = np.zeros((max_len), dtype=np.int64)\n",
        "    if len(x) > max_len: padded[:] = x[:max_len]\n",
        "    else: padded[:len(x)] = x\n",
        "    return padded"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnsursG4RxLm",
        "outputId": "17d5e560-b4ff-4b5e-dd1f-b64a0d92494d"
      },
      "source": [
        "# inplace padding\n",
        "input_tensor = [pad_sequences(x, max_length_inp) for x in input_tensor]\n",
        "target_tensor = [pad_sequences(x, max_length_tar) for x in target_tensor]\n",
        "len(target_tensor)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JnGTooeRyx_",
        "outputId": "4fc9d41f-bcbb-45c7-9fc3-6ce5693cae07"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2400, 2400, 600, 600)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RayN_kaSR0Hm"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK3KD3PHR2yS"
      },
      "source": [
        "# conver the data to tensors and pass to the Dataloader \n",
        "# to create an batch iterator\n",
        "\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        # TODO: convert this into torch code is possible\n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x,y,x_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ-czHKWR4TD"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "train_dataset = MyData(input_tensor_train, target_tensor_train)\n",
        "val_dataset = MyData(input_tensor_val, target_tensor_val)\n",
        "\n",
        "dataset = DataLoader(train_dataset, batch_size = BATCH_SIZE, \n",
        "                     drop_last=True,\n",
        "                     shuffle=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLmsHDDetcuy"
      },
      "source": [
        "**Test Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jknAX2bktbEq"
      },
      "source": [
        "import torch.nn as nn\n",
        "class EncodeDecode(nn.Module):\n",
        "  def __init__(self, vocab_inp_size, embedding_dim, enc_units, dec_units,batch_size):\n",
        "    super(EncodeDecode, self).__init__()\n",
        "    self.batch_sz = batch_size\n",
        "    self.enc_units = enc_units\n",
        "    self.dec_units = dec_units\n",
        "    self.vocab_size = vocab_inp_size\n",
        "    # self.vocab_tar_size = vocab_tar_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "    self.gruEn = nn.GRU(self.embedding_dim, self.enc_units)\n",
        "    self.gruDec = nn.GRU(self.embedding_dim + self.enc_units, self.dec_units,\n",
        "                         batch_first=True)\n",
        "    self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
        "    \n",
        "    # used for attention\n",
        "    self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
        "    self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
        "    self.V = nn.Linear(self.enc_units, 1)\n",
        "\n",
        "  def forward_enc(self, x, lens, device):\n",
        "      x = self.embedding(x) \n",
        "      \n",
        "      x = pack_padded_sequence(x, lens) # unpad\n",
        "    \n",
        "      self.hidden = self.initialize_hidden_state(device)\n",
        "        \n",
        "      output, self.hidden = self.gru(x, self.hidden) # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
        "      \n",
        "      # pad the sequence to the max length in the batch\n",
        "      output, _ = pad_packed_sequence(output)\n",
        "      \n",
        "      return output, self.hidden\n",
        "  def forward(self, x, hidden, enc_output):\n",
        "        # enc_output original: (max_length, batch_size, enc_units)\n",
        "        # enc_output converted == (batch_size, max_length, hidden_size)\n",
        "      enc_output = enc_output.permute(1,0,2)\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "      hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
        "        \n",
        "        # score: (batch_size, max_length, hidden_size) # Bahdanaus's\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        # It doesn't matter which FC we pick for each of the inputs\n",
        "      score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "        \n",
        "        #score = torch.tanh(self.W2(hidden_with_time_axis) + self.W1(enc_output))\n",
        "          \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "      attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "      context_vector = attention_weights * enc_output\n",
        "      context_vector = torch.sum(context_vector, dim=1)\n",
        "      \n",
        "      # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "      # takes case of the right portion of the model above (illustrated in red)\n",
        "      x = self.embedding(x)\n",
        "      \n",
        "      # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "      #x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "      # ? Looks like attention vector in diagram of source\n",
        "      x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
        "      \n",
        "      # passing the concatenated vector to the GRU\n",
        "      # output: (batch_size, 1, hidden_size)\n",
        "      output, state = self.gru(x)\n",
        "      \n",
        "      \n",
        "      # output shape == (batch_size * 1, hidden_size)\n",
        "      output =  output.view(-1, output.size(2))\n",
        "      \n",
        "      # output shape == (batch_size * 1, vocab)\n",
        "      x = self.fc(output)\n",
        "      \n",
        "      return x, state, attention_weights\n",
        "\n",
        "  def initialize_hidden_state(self, device):\n",
        "    return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsgV-RAVtg1h"
      },
      "source": [
        "**END TEST CODE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfHuZCIZR5vo"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
        "        \n",
        "    def forward(self, x, lens, device):\n",
        "        # x: batch_size, max_length \n",
        "        \n",
        "        # x: batch_size, max_length, embedding_dim\n",
        "        x = self.embedding(x) \n",
        "                \n",
        "        # x transformed = max_len X batch_size X embedding_dim\n",
        "        # x = x.permute(1,0,2)\n",
        "        x = pack_padded_sequence(x, lens) # unpad\n",
        "    \n",
        "        self.hidden = self.initialize_hidden_state(device)\n",
        "        \n",
        "        # output: max_length, batch_size, enc_units\n",
        "        # self.hidden: 1, batch_size, enc_units\n",
        "        output, self.hidden = self.gru(x, self.hidden) # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
        "        \n",
        "        # pad the sequence to the max length in the batch\n",
        "        output, _ = pad_packed_sequence(output)\n",
        "        \n",
        "        return output, self.hidden\n",
        "\n",
        "    def initialize_hidden_state(self, device):\n",
        "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xzVF502R7hP"
      },
      "source": [
        "### sort batch function to be able to use with pad_packed_sequence\n",
        "def sort_batch(X, y, lengths):\n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]\n",
        "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDRTMIBeR8yO",
        "outputId": "aa9ce13f-2854-432f-9535-dc2c52498a46"
      },
      "source": [
        "### Testing Encoder part\n",
        "# TODO: put whether GPU is available or not\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "\n",
        "print(enc_output.size()) # max_length, batch_size, enc_units"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([7, 64, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP7mfzEIR-Qp"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim + self.enc_units, \n",
        "                          self.dec_units,\n",
        "                          batch_first=True)\n",
        "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.V = nn.Linear(self.enc_units, 1)\n",
        "    \n",
        "    def forward(self, x, hidden, enc_output):\n",
        "        # enc_output original: (max_length, batch_size, enc_units)\n",
        "        # enc_output converted == (batch_size, max_length, hidden_size)\n",
        "        enc_output = enc_output.permute(1,0,2)\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
        "        \n",
        "        # score: (batch_size, max_length, hidden_size) # Bahdanaus's\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        # It doesn't matter which FC we pick for each of the inputs\n",
        "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "        \n",
        "        #score = torch.tanh(self.W2(hidden_with_time_axis) + self.W1(enc_output))\n",
        "          \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        # takes case of the right portion of the model above (illustrated in red)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        #x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        # ? Looks like attention vector in diagram of source\n",
        "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        # output: (batch_size, 1, hidden_size)\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output =  output.view(-1, output.size(2))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return torch.zeros((1, self.batch_sz, self.dec_units))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSzf5rGUSCKm",
        "outputId": "d64f324a-f800-459d-9432-b5ae38fb7f3b"
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "print(\"Input: \", x.shape)\n",
        "print(\"Output: \", y.shape)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "print(\"Encoder Output: \", enc_output.shape) # batch_size X max_length X enc_units\n",
        "print(\"Encoder Hidden: \", enc_hidden.shape) # batch_size X enc_units (corresponds to the last state)\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "#print(enc_hidden.squeeze(0).shape)\n",
        "\n",
        "dec_hidden = enc_hidden#.squeeze(0)\n",
        "dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "print(\"Decoder Input: \", dec_input.shape)\n",
        "print(\"--------\")\n",
        "\n",
        "for t in range(1, y.size(1)):\n",
        "    # enc_hidden: 1, batch_size, enc_units\n",
        "    # output: max_length, batch_size, enc_units\n",
        "    predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "    \n",
        "    print(\"Prediction: \", predictions.shape)\n",
        "    print(\"Decoder Hidden: \", dec_hidden.shape)\n",
        "    \n",
        "    #loss += loss_function(y[:, t].to(device), predictions.to(device))\n",
        "    \n",
        "    dec_input = y[:, t].unsqueeze(1)\n",
        "    print(dec_input.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  torch.Size([64, 8])\n",
            "Output:  torch.Size([64, 9])\n",
            "Encoder Output:  torch.Size([4, 64, 1024])\n",
            "Encoder Hidden:  torch.Size([1, 64, 1024])\n",
            "Decoder Input:  torch.Size([64, 1])\n",
            "--------\n",
            "Prediction:  torch.Size([64, 1474])\n",
            "Decoder Hidden:  torch.Size([1, 64, 1024])\n",
            "torch.Size([64, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tw1XyqiSDpP"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
        "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
        "    #print(mask)\n",
        "    mask = real.ge(1).type(torch.FloatTensor)\n",
        "    \n",
        "    loss_ = criterion(pred, real) * mask \n",
        "    return torch.mean(loss_)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqQPrc45WO8-"
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "## TODO: Combine the encoder and decoder into one class\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
        "                       lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7LlXiiUWRH8"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ, inp_len)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "        enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "        dec_hidden = enc_hidden\n",
        "        \n",
        "        # use teacher forcing - feeding the target as the next input (via dec_input)\n",
        "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "        \n",
        "        # run code below for every timestep in the ys batch\n",
        "        for t in range(1, ys.size(1)):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "            loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
        "            #loss += loss_\n",
        "            dec_input = ys[:, t].unsqueeze(1)\n",
        "            \n",
        "        \n",
        "        batch_loss = (loss / int(ys.size(1)))\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.detach().item()))\n",
        "        \n",
        "        \n",
        "    ### TODO: Save checkpoint for model\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxsTpmj_x5Wi"
      },
      "source": [
        "**More Test Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BnPOXpWyI73"
      },
      "source": [
        "######"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kED7oh88yGtg"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
        "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
        "    #print(mask)\n",
        "    mask = real.ge(1).type(torch.FloatTensor)\n",
        "    \n",
        "    loss_ = criterion(pred, real) * mask \n",
        "    return torch.mean(loss_)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGGMxt99yEIW"
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "## TODO: Combine the encoder and decoder into one class\n",
        "# encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "# decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "batch_sz = BATCH_SIZE\n",
        "model = EncodeDecode(vocab_inp_size, embedding_dim, enc_units= units, dec_units= units, batch_size=batch_sz)\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(list(model.parameters()), lr=0.001)\n",
        "# optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
        "#                        lr=0.001)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT9puu3jWTO_"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    model.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ, inp_len)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "        enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "        dec_hidden = enc_hidden\n",
        "        \n",
        "        # use teacher forcing - feeding the target as the next input (via dec_input)\n",
        "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "        \n",
        "        # run code below for every timestep in the ys batch\n",
        "        for t in range(1, ys.size(1)):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "            loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
        "            #loss += loss_\n",
        "            dec_input = ys[:, t].unsqueeze(1)\n",
        "            \n",
        "        \n",
        "        batch_loss = (loss / int(ys.size(1)))\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.detach().item()))\n",
        "        \n",
        "        \n",
        "    ### TODO: Save checkpoint for model\n",
        "\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "    path = 'model.pt'\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': decoder.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': total_loss\n",
        "    }, path)\n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 4.5420\n",
            "Epoch 1 Loss 4.6519\n",
            "Time taken for 1 epoch 37.873233795166016 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.5709\n",
            "Epoch 2 Loss 4.6525\n",
            "Time taken for 1 epoch 39.56311869621277 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.6661\n",
            "Epoch 3 Loss 4.6529\n",
            "Time taken for 1 epoch 40.04192519187927 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 4.6390\n",
            "Epoch 4 Loss 4.6536\n",
            "Time taken for 1 epoch 39.400871992111206 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 4.7135\n",
            "Epoch 5 Loss 4.6532\n",
            "Time taken for 1 epoch 38.99322485923767 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 4.6100\n",
            "Epoch 6 Loss 4.6560\n",
            "Time taken for 1 epoch 40.58640909194946 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 4.5899\n",
            "Epoch 7 Loss 4.6543\n",
            "Time taken for 1 epoch 39.36858797073364 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 4.5511\n",
            "Epoch 8 Loss 4.6544\n",
            "Time taken for 1 epoch 38.95696711540222 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 4.6160\n",
            "Epoch 9 Loss 4.6532\n",
            "Time taken for 1 epoch 37.63916516304016 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 4.4987\n",
            "Epoch 10 Loss 4.6553\n",
            "Time taken for 1 epoch 37.84735584259033 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}